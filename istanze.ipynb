{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definizione delle funzioni e classi:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utilizzo di più istanze sugli algoritmi di clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libreries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import community as community_louvain \n",
    "from node2vec import Node2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import time,random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Louvain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(graph, title, subplot_position=None, filename=None, colors=None):\n",
    "    plt.subplot(subplot_position) if subplot_position else plt.figure()\n",
    "    pos = nx.spring_layout(graph, seed=42)\n",
    "    labels = {node: str(node) for node in graph.nodes()}\n",
    "    node_colors = {node: 'skyblue' for node in graph.nodes()}\n",
    "\n",
    "    if colors:\n",
    "        color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "        for i, cluster in enumerate(colors):\n",
    "            color = next(color_cycle)\n",
    "            for node in cluster:\n",
    "                node_colors[int(node)] = color\n",
    "\n",
    "    nx.set_node_attributes(graph, node_colors, 'color')\n",
    "\n",
    "    nx.draw(graph, pos, with_labels=True, labels=labels, node_size=1000, node_color=list(node_colors.values()), \n",
    "            font_size=12, font_color='black', edge_color='gray', edgecolors='black', linewidths=1)\n",
    "\n",
    "    plt.title(title)\n",
    "    if filename and not subplot_position:\n",
    "        plt.savefig(filename)\n",
    "    if not subplot_position:\n",
    "        plt.show()\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.grafo = {}\n",
    "\n",
    "    def aggiungi_vertice(self, vertice):\n",
    "        if vertice not in self.grafo:\n",
    "            self.grafo[vertice] = []\n",
    "\n",
    "    def aggiungi_arco(self, u, v):\n",
    "        if u in self.grafo and v in self.grafo:\n",
    "            self.grafo[u].append(v)\n",
    "            self.grafo[v].append(u)\n",
    "\n",
    "    def rimuovi_vertice(self, vertice):\n",
    "        if vertice in self.grafo:\n",
    "            self.grafo.pop(vertice, None)\n",
    "            for nodo, adiacenti in self.grafo.items():\n",
    "                if vertice in adiacenti:\n",
    "                    adiacenti.remove(vertice)\n",
    "\n",
    "\n",
    "    def stampa_grafo(self):\n",
    "        for vertice in self.grafo:\n",
    "            adiacenti = \", \".join(str(v) for v in self.grafo[vertice])\n",
    "            print(f\"{vertice} -> {adiacenti}\")\n",
    "\n",
    "def converti_grafo_personalizzato_in_networkx(grafo_personalizzato):\n",
    "    G = nx.Graph()\n",
    "    for vertice, adiacenti in grafo_personalizzato.grafo.items():\n",
    "        G.add_node(vertice)\n",
    "        for adiacente in adiacenti:\n",
    "            G.add_edge(vertice, adiacente)\n",
    "    return G\n",
    "\n",
    "\n",
    "def divide_into_clusters(graph):\n",
    "    # 'partition' è un dizionario dove le chiavi sono i nodi del grafo e i valori\n",
    "    # sono gli identificatori delle comunità (cluster) a cui i nodi appartengono.\n",
    "    partition = community_louvain.best_partition(graph)\n",
    "    # Crea un defaultdict (una sottoclasse di dict) che produce un comportamento\n",
    "    # predefinito per l'accesso a chiavi non esistenti, utilizzando list come\n",
    "    # factory. Qui 'clusters' sarà un dizionario dove le chiavi sono gli identificatori\n",
    "    # delle comunità e i valori sono liste di nodi appartenenti a quella comunità.\n",
    "    clusters = defaultdict(list)\n",
    "    # Itera su tutti gli elementi in 'partition'. 'node' è un nodo nel grafo\n",
    "    # e 'cluster_index' è la comunità (cluster) a cui il nodo è assegnato.\n",
    "    for node, cluster_index in partition.items():\n",
    "        # Aggiunge il nodo alla lista di nodi della sua comunità nel dizionario\n",
    "        # 'clusters'. La comunità è identificata da 'cluster_index'.\n",
    "        clusters[cluster_index].append(node)\n",
    "    # Ritorna le liste di nodi raggruppati in comunità come una lista di liste.\n",
    "    # Ogni sottolista contiene tutti i nodi appartenenti a una comunità.\n",
    "    return list(clusters.values())\n",
    "\n",
    "def divide_into_clusters_fixed_k(graph, k):\n",
    "    # ottieniamo una divisione iniziale dei nodi del grafo in cluster basata sull'algoritmo di Louvain.\n",
    "    clusters = divide_into_clusters(graph)\n",
    "\n",
    "    # Se ci sono già k cluster, restituirli\n",
    "    if len(clusters) == k:\n",
    "        return clusters\n",
    "\n",
    "    # Se ci sono più di k cluster, combinare quelli più piccoli\n",
    "    while len(clusters) > k:\n",
    "        # Trovare il cluster più piccolo\n",
    "        smallest_cluster = min(clusters, key=len)\n",
    "        clusters.remove(smallest_cluster)\n",
    "        # Trovare il cluster con cui il più piccolo condivide il maggior numero di archi.\n",
    "        max_shared_edges = 0 # Numero di archi condivisi\n",
    "        best_cluster_to_combine = None # Il cluster con cui combinare il più piccolo\n",
    "        for cluster in clusters: # Itera su tutti i cluster\n",
    "            shared_edges = sum(1 for node in smallest_cluster if set(graph.neighbors(node)) & set(cluster)) # Numero di archi condivisi con il più piccolo\n",
    "            if shared_edges > max_shared_edges: # Se il numero di archi condivisi è maggiore del massimo finora\n",
    "                max_shared_edges = shared_edges # Aggiorna il massimo \n",
    "                best_cluster_to_combine = cluster # Aggiorna il cluster migliore\n",
    "        # Combina il più piccolo con il migliore\n",
    "        best_cluster_to_combine.extend(smallest_cluster)\n",
    "\n",
    "    # Se ci sono meno di k cluster, dividere quelli più grandi\n",
    "    while len(clusters) < k:\n",
    "        # Find the largest cluster\n",
    "        largest_cluster = max(clusters, key=len)\n",
    "        clusters.remove(largest_cluster)\n",
    "        # Trova il nodo con il maggior numero di connessioni al di fuori del cluster\n",
    "        max_external_connections = 0\n",
    "        best_node_to_move = None\n",
    "        for node in largest_cluster:\n",
    "            external_connections = sum(1 for neighbor in graph.neighbors(node) if neighbor not in largest_cluster)\n",
    "            if external_connections > max_external_connections:\n",
    "                max_external_connections = external_connections\n",
    "                best_node_to_move = node\n",
    "        # Crea un nuovo cluster con il nodo migliore\n",
    "        new_cluster = [best_node_to_move]\n",
    "        if best_node_to_move in largest_cluster:\n",
    "            largest_cluster.remove(best_node_to_move)\n",
    "        #largest_cluster.remove(best_node_to_move)\n",
    "        # Spostare i nodi collegati al nodo del nuovo cluster dal vecchio al nuovo cluster.\n",
    "        for node in list(largest_cluster):  # Copiare l'elenco per evitare problemi durante la rimozione degli elementi\n",
    "            if best_node_to_move in graph.neighbors(node):\n",
    "                new_cluster.append(node)\n",
    "                largest_cluster.remove(node)\n",
    "        # Add the modified and new clusters\n",
    "        clusters.append(largest_cluster)\n",
    "        clusters.append(new_cluster)\n",
    "\n",
    "    # Rimuovi i cluster vuoti (se ce ne sono)\n",
    "    clusters = [cluster for cluster in clusters if cluster]\n",
    "\n",
    "    return clusters \n",
    "\n",
    "\n",
    "\n",
    "# def compute_modularity(graph, partition):\n",
    "#     A = nx.adjacency_matrix(graph) # Matrice di adiacenza del grafo \n",
    "#     m = graph.number_of_edges() #  Numero di archi\n",
    "#     nodes = graph.nodes() # Lista dei nodi\n",
    "    \n",
    "#     Q = 0 # inizializzazione della modularità a 0\n",
    "#     for i in range(len(nodes)): # Itera su tutti i nodi\n",
    "#         ki = graph.degree(nodes[i]) # \n",
    "#         ci = partition[i] # \n",
    "#         for j in range(len(nodes)):\n",
    "#             kj = graph.degree(nodes[j]) #\n",
    "#             cj = partition[j] # \n",
    "#             if ci == cj: #\n",
    "#                 Q += A[i,j] - (ki * kj) / (2 * m)\n",
    "#     Q = Q / (2 * m)\n",
    "#     return Q\n",
    "\n",
    "def calculate_multicut_size(graph, clusters):\n",
    "    multicut_size = 0\n",
    "    for cluster in clusters:\n",
    "        for node in cluster:\n",
    "            for neighbor in graph.neighbors(node):\n",
    "                if neighbor not in cluster:\n",
    "                    multicut_size += 1\n",
    "    # Poiché ogni arco viene contato due volte (una volta per ogni estremo), dividi per 2\n",
    "    return multicut_size // 2\n",
    "\n",
    "\n",
    "def è_ammissibile(grafo, clusters, k):\n",
    "    # Vincolo 2: numero di sottografi = k\n",
    "    if len(clusters) != k:\n",
    "        return False, \"Vincolo 2 violato: il numero di sottografi non è k.\"\n",
    "\n",
    "  # Vincolo 1: Ogni vertice in uno e solo uno dei sottografi\n",
    "    tutti_nodi = set(grafo.nodes())\n",
    "    union_clusters = set().union(*clusters)\n",
    "    missing_nodi = tutti_nodi - union_clusters\n",
    "    extra_nodi = union_clusters - tutti_nodi\n",
    "\n",
    "    if missing_nodi:\n",
    "        return False, f\"Vincolo 1 violato: i nodi {missing_nodi} non sono presenti in nessun cluster.\"\n",
    "    elif extra_nodi:\n",
    "        return False, f\"Vincolo 1 violato: i nodi {extra_nodi} sono presenti in più cluster.\"\n",
    "\n",
    "\n",
    "    # Vincolo 3: verifica l'integrità e la connettività dei cluster\n",
    "    for cluster in clusters:\n",
    "        if not cluster:\n",
    "            return False, \"Vincolo 3 violato: cluster vuoto.\"\n",
    "        \n",
    "\n",
    "    return True, \"Ammissibile\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def calculate_multicut_size_with_partition(graph, partition):\n",
    "    # Converti la partizione in una lista di cluster\n",
    "    clusters = [[] for _ in range(max(partition.values()) + 1)]\n",
    "    for node, cluster_id in partition.items():\n",
    "        clusters[cluster_id].append(node)\n",
    "    \n",
    "    # Il resto della funzione rimane invariato\n",
    "    multicut_size = 0\n",
    "    for cluster in clusters:\n",
    "        for node in cluster:\n",
    "            for neighbor in graph.neighbors(node):\n",
    "                if neighbor not in cluster:\n",
    "                    multicut_size += 1\n",
    "    return multicut_size // 2 \"\"\"\n",
    "    \n",
    "def calculate_multicut_size_with_partition(graph, partition):\n",
    "    multicut_size = 0\n",
    "    for node in graph.nodes():\n",
    "        for neighbor in graph.neighbors(node):\n",
    "            if partition[node] != partition[neighbor]:\n",
    "                multicut_size += 1\n",
    "    return multicut_size // 2\n",
    "\n",
    "\n",
    "\n",
    "def calculate_modularity(graph, clusters, partition):\n",
    "    \"\"\"\n",
    "    Calcola la modularità di una certa partizione di un grafo.\n",
    "    \n",
    "    Parametri:\n",
    "    - graph: networkx.Graph, il grafo su cui calcolare la modularità.\n",
    "    - clusters: list of lists, la partizione del grafo.\n",
    "    \n",
    "    Ritorna:\n",
    "    - Q: float, il valore della modularità.\n",
    "    \"\"\"\n",
    "    # Calcola il grado di ogni nodo\n",
    "    degrees = dict(graph.degree())\n",
    "    \n",
    "    # Mappa ogni nodo al suo cluster usando la partizione passata come parametro\n",
    "    node_to_cluster = partition\n",
    "    # Calcola m\n",
    "    m = len(graph.edges())\n",
    "    \n",
    "    # Inizializza Q a 0\n",
    "    Q = 0\n",
    "    \n",
    "    # Mappa ogni nodo al suo cluster\n",
    "    node_to_cluster = {node: cluster_id for cluster_id, cluster in enumerate(clusters) for node in cluster}\n",
    "    \n",
    "    # Calcola Q\n",
    "    for i in graph.nodes():\n",
    "        ki = degrees[i]  # Grado del nodo i\n",
    "        for j in graph.nodes():\n",
    "            Aij = 1 if graph.has_edge(i, j) else 0  # Aij è 1 se esiste un bordo tra i e j, altrimenti è 0\n",
    "            kj = degrees[j]  # Grado del nodo j\n",
    "            \n",
    "            # δ(ci, cj) è 1 se i e j sono nello stesso cluster, altrimenti è 0\n",
    "            delta = 1 if node_to_cluster[i] == node_to_cluster[j] else 0 \n",
    "            \n",
    "            # Aggiorna Q secondo la formula della modularità\n",
    "            Q += (Aij - (ki * kj) / (2 * m)) * delta\n",
    "            \n",
    "    # Normalizza Q\n",
    "    Q = Q / (2 * m)\n",
    "    \n",
    "    return Q\n",
    "\n",
    "\n",
    "def local_search(graph, partition):\n",
    "    \"\"\" Esegue la ricerca locale per migliorare il taglio minimo della partizione. \"\"\"\n",
    "    current_cut = calculate_multicut_size_with_partition(graph, partition)\n",
    "    improvement = True\n",
    "    \n",
    "    while improvement:\n",
    "        improvement = False\n",
    "        for node in graph.nodes():\n",
    "            # Salva la partizione originale del nodo\n",
    "            original_partition = partition[node]\n",
    "            best_cut = current_cut\n",
    "            best_partition = original_partition\n",
    "            \n",
    "            # Verifica se spostare il nodo in un'altra partizione migliorerebbe il taglio\n",
    "            for new_partition in set(partition.values()):\n",
    "                if new_partition != original_partition:\n",
    "                    partition[node] = new_partition\n",
    "                    new_cut = calculate_multicut_size_with_partition(graph, partition)\n",
    "                    if new_cut < best_cut:\n",
    "                        best_cut = new_cut\n",
    "                        best_partition = new_partition\n",
    "                        improvement = True\n",
    "            \n",
    "            # Aggiorna la partizione del nodo\n",
    "            partition[node] = best_partition\n",
    "            \n",
    "            # Aggiorna il miglior taglio trovato\n",
    "            current_cut = best_cut\n",
    "    \n",
    "    return partition, current_cut\n",
    "\n",
    "def louvain_with_local_search(graph, initial_clusters, k):\n",
    "    # Trasforma i cluster iniziali in una partizione (un dizionario nodo -> id_cluster)\n",
    "    partition = {}\n",
    "    for cluster_id, cluster in enumerate(initial_clusters):\n",
    "        for node in cluster:\n",
    "            partition[node] = cluster_id\n",
    "    \n",
    "    improvement = True\n",
    "    while improvement:\n",
    "        improvement = False\n",
    "        for node in graph.nodes():\n",
    "            current_cluster = partition[node]\n",
    "            best_cluster = current_cluster\n",
    "            best_multicut_size = calculate_multicut_size_with_partition(graph, partition)\n",
    "            \n",
    "            # Prova a spostare 'node' in un cluster diverso e verifica se c'è un miglioramento\n",
    "            for new_cluster_id in range(k):\n",
    "                if new_cluster_id == current_cluster:\n",
    "                    continue\n",
    "                \n",
    "                # Controlla se lo spostamento lascerebbe il cluster originale vuoto\n",
    "                if list(partition.values()).count(current_cluster) == 1:\n",
    "                    continue\n",
    "                \n",
    "                # Sposta temporaneamente il nodo al nuovo cluster\n",
    "                partition[node] = new_cluster_id\n",
    "                new_multicut_size = calculate_multicut_size_with_partition(graph, partition)\n",
    "                \n",
    "                # Controlla se il nuovo taglio multicut è minore\n",
    "                if new_multicut_size < best_multicut_size:\n",
    "                    best_multicut_size = new_multicut_size\n",
    "                    best_cluster = new_cluster_id\n",
    "                    improvement = True\n",
    "                    \n",
    "                # Ripristina la posizione originale del nodo per le prossime iterazioni\n",
    "                partition[node] = current_cluster\n",
    "            \n",
    "            # Aggiorna la posizione del nodo se abbiamo trovato un cluster migliore\n",
    "            partition[node] = best_cluster\n",
    "    \n",
    "    # Trasforma la partizione finale in una lista di cluster e restituiscila\n",
    "    final_clusters = [[] for _ in range(k)]\n",
    "    for node, cluster_id in partition.items():\n",
    "        final_clusters[cluster_id].append(node)\n",
    "    \n",
    "    # Rimuove eventuali cluster vuoti (non dovrebbe essercene, ma per sicurezza)\n",
    "    final_clusters = [cluster for cluster in final_clusters if cluster]\n",
    "    \n",
    "    return final_clusters\n",
    "\n",
    "def local_search_optimize_modularity(graph, initial_clusters, k):\n",
    "    partition = {node: cluster_id for cluster_id, cluster in enumerate(initial_clusters) for node in cluster}\n",
    "    \n",
    "    improvement = True\n",
    "    while improvement:\n",
    "        improvement = False\n",
    "        for node in graph.nodes():\n",
    "            current_cluster = partition[node]\n",
    "            best_cluster = current_cluster\n",
    "            best_modularity = calculate_modularity(graph, [cluster for cluster_id, cluster in enumerate(initial_clusters) if cluster_id != current_cluster] + [[node] + [n for n in initial_clusters[current_cluster] if n != node]], partition)\n",
    "            \n",
    "            for new_cluster_id in set(partition.values()):\n",
    "                if new_cluster_id == current_cluster:\n",
    "                    continue\n",
    "                \n",
    "                # Sposta temporaneamente il nodo al nuovo cluster\n",
    "                partition[node] = new_cluster_id\n",
    "                new_modularity = calculate_modularity(graph, [cluster for cluster_id, cluster in enumerate(initial_clusters) if cluster_id != current_cluster] + [[node] + [n for n in initial_clusters[new_cluster_id]]], partition)\n",
    "                \n",
    "                # Controlla se la nuova modularità è maggiore\n",
    "                if new_modularity > best_modularity:\n",
    "                    best_modularity = new_modularity\n",
    "                    best_cluster = new_cluster_id\n",
    "                    improvement = True\n",
    "                    \n",
    "                # Ripristina la posizione originale del nodo per le prossime iterazioni\n",
    "                partition[node] = current_cluster\n",
    "            \n",
    "            # Aggiorna la posizione del nodo se abbiamo trovato un cluster migliore\n",
    "            partition[node] = best_cluster\n",
    "            \n",
    "            # Aggiorna la posizione del nodo se abbiamo trovato un cluster migliore\n",
    "            partition[node] = best_cluster\n",
    "    \n",
    "    # Trasforma la partizione finale in una lista di cluster e restituiscila\n",
    "    final_clusters = [[] for _ in range(k)]\n",
    "    for node, cluster_id in partition.items():\n",
    "        final_clusters[cluster_id].append(node)\n",
    "    \n",
    "    # Rimuove eventuali cluster vuoti\n",
    "    final_clusters = [cluster for cluster in final_clusters if cluster]\n",
    "    \n",
    "    return final_clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmi genetici\n",
    "\n",
    "Utilizzare gli algoritmi genetici (GA) per risolvere il problema di clustering può richiedere alcune decisioni di design:\n",
    "\n",
    "- Codifica: come rappresentare una soluzione come un individuo in una popolazione.\n",
    "- Funzione di fitness: come valutare la bontà di una soluzione.\n",
    "- Selezione: come selezionare gli individui per la riproduzione.\n",
    "- Crossover (ricombinazione): come combinare due individui per produrre un nuovo individuo.\n",
    "- Mutazione: come introdurre piccole variazioni in un individuo.\n",
    "  \n",
    "Riorganizzando il codice per utilizzare gli algoritmi genetici:\n",
    "\n",
    "- Codifica: ogni individuo è una lista di lunghezza uguale al numero di nodi nel grafo, dove l'elemento i-esimo rappresenta l'ID del cluster per il nodo i.\n",
    "\n",
    "- Funzione di fitness: la funzione di fitness può essere basata sulla dimensione del multicut. Valori più bassi sono migliori.\n",
    "\n",
    "- Selezione: utilizziamo la selezione della ruota della roulette.\n",
    "\n",
    "- Crossover: utilizziamo un crossover a punto singolo.\n",
    "\n",
    "- Mutazione: cambiamo l'ID del cluster di un nodo in modo casuale.\n",
    "\n",
    "\n",
    "\n",
    "Per ripristinare l'ammissibilità, è necessario seguire una procedura dopo la mutazione per garantire che tutti i vincoli siano rispettati. Una procedura potrebbe essere:\n",
    "\n",
    "Assicurarsi che ogni individuo abbia esattamente k clusters.\n",
    "Assicurarsi che ogni nodo del grafo sia presente in uno ed esattamente un cluster.\n",
    "Assicurarsi che ogni cluster sia connesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "\n",
    "\n",
    "def calcola_dimensione_multicut(grafo, individual):\n",
    "    node_to_cluster = {node: cluster for cluster, nodes in enumerate(individual) for node in nodes}\n",
    "    dimensione_multicut = 0\n",
    "    for u, v in grafo.edges():\n",
    "        if node_to_cluster[u] == node_to_cluster[v]:\n",
    "            continue\n",
    "        dimensione_multicut += 1\n",
    "    return dimensione_multicut\n",
    "\n",
    "def init_population(size, num_nodes, k): # provare a inizializzare la greedy al posto di random\n",
    "    return [[random.randint(0, k-1) for _ in range(num_nodes)] for _ in range(size)]\n",
    "\n",
    "\"\"\" def fitness(individual, graph, k):\n",
    "    clusters = [[] for _ in range(max(individual) + 1)]\n",
    "    for i, cluster_id in enumerate(individual):\n",
    "        clusters[cluster_id].append(i)\n",
    "    \n",
    "    fitness_val = calcola_dimensione_multicut(graph, clusters)\n",
    "    \n",
    "    # Penalizzazione per soluzioni non ammissibili\n",
    "    if not è_ammissibile(graph, clusters, k):\n",
    "        fitness_val += 1e6 # Una penalizzazione molto grande\n",
    "    \n",
    "    return fitness_val \"\"\"\n",
    "def fitness(individual, graph, k):\n",
    "    clusters = [[] for _ in range(max(individual) + 1)]\n",
    "    for i, cluster_id in enumerate(individual):\n",
    "        clusters[cluster_id].append(i)\n",
    "    \n",
    "    # Verifica l'ammissibilità della soluzione\n",
    "    is_feasible, violation_message = è_ammissibile(graph, clusters, k)\n",
    "    \n",
    "    if not is_feasible:\n",
    "        return 1e6  # Una penalizzazione molto grande se la soluzione non è ammissibile\n",
    "    \n",
    "    dimensione_multicut = calcola_dimensione_multicut(graph, clusters)\n",
    "    \n",
    "    return dimensione_multicut\n",
    "\n",
    "\n",
    "\n",
    "def roulette_wheel_selection(population, fitness_values):\n",
    "    cumulative_fitness = [sum(fitness_values[:i+1]) for i in range(len(fitness_values))]\n",
    "    pick = random.uniform(0, cumulative_fitness[-1])\n",
    "    chosen_index = bisect.bisect_left(cumulative_fitness, pick)\n",
    "    return population[chosen_index]\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    idx = random.randint(0, len(parent1) - 1)\n",
    "    child1 = parent1[:idx] + parent2[idx:]\n",
    "    child2 = parent2[:idx] + parent1[idx:]\n",
    "    return child1, child2\n",
    "\n",
    "def mutate(individual, k):\n",
    "    idx = random.randint(0, len(individual) - 1)\n",
    "    new_cluster = random.randint(0, k-1)\n",
    "    while new_cluster == individual[idx]:\n",
    "        new_cluster = random.randint(0, k-1)\n",
    "    individual[idx] = new_cluster\n",
    "\n",
    "def genetic_algorithm(graph, k, pop_size=100, gen_count=1000, mutation_rate=0.01):\n",
    "    population = init_population(pop_size, len(graph.nodes()), k)\n",
    "    fitness_values = [fitness(ind, graph, k) for ind in population]\n",
    "    elitism_size = int(0.10 * len(population))\n",
    "    \n",
    "    for gen in range(gen_count):\n",
    "        new_population = []\n",
    "        elite_indices = sorted(range(len(fitness_values)), key=lambda x: fitness_values[x])[:elitism_size]\n",
    "        \n",
    "        for elite_index in elite_indices:\n",
    "            new_population.append(population[elite_index])\n",
    "            \n",
    "        while len(new_population) < pop_size:\n",
    "            parent1 = roulette_wheel_selection(population, fitness_values)\n",
    "            parent2 = roulette_wheel_selection(population, fitness_values)\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            \n",
    "            if random.random() < mutation_rate:\n",
    "                mutate(child1, k)\n",
    "            if random.random() < mutation_rate:\n",
    "                mutate(child2, k)\n",
    "            new_population.extend([child1, child2])\n",
    "\n",
    "        fitness_values = [fitness(ind, graph, k) for ind in population]\n",
    "\n",
    "        population = new_population\n",
    "    \n",
    "    best_individual = min(population, key=lambda ind: fitness(ind, graph, k))\n",
    "    clusters = [[] for _ in range(max(best_individual) + 1)]\n",
    "    for i, cluster_id in enumerate(best_individual):\n",
    "        clusters[cluster_id].append(i)\n",
    "    return clusters\n",
    "\n",
    "def è_ammissibile(grafo, clusters, k):\n",
    "    # Vincolo 2: numero di sottografi = k\n",
    "    if len(clusters) != k:\n",
    "        return False\n",
    "\n",
    " # Vincolo 1: Ogni vertice in uno e solo uno dei sottografi\n",
    "    tutti_nodi = set(grafo.nodes())\n",
    "    union_clusters = set().union(*clusters)\n",
    "    missing_nodi = tutti_nodi - union_clusters\n",
    "    extra_nodi = union_clusters - tutti_nodi\n",
    "\n",
    "    if missing_nodi:\n",
    "        return False, f\"Vincolo 1 violato: i nodi {missing_nodi} non sono presenti in nessun cluster.\"\n",
    "    elif extra_nodi:\n",
    "        return False, f\"Vincolo 1 violato: i nodi {extra_nodi} sono presenti in più cluster.\"\n",
    "\n",
    "\n",
    "    return True, \"Ammisibile\"\n",
    "\n",
    "\n",
    "# Quando la sol viene aggiunta al pop, se non è ammissibile, la si corregge -> in principal modo se il cluster è vuoto togliere il nodo e metterlo dentro a un altro cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_search_best_improvement_genetic(solution, graph, k):\n",
    "    best_solution = solution[:] # Copia profonda\n",
    "    best_fitness = fitness(solution, graph, k) # Calcola il fitness della soluzione iniziale\n",
    "\n",
    "    improved = True # indica se abbiamo trovato un miglioramento\n",
    "    while improved: # Continua finché si trovano miglioramenti\n",
    "        improved = False # Resetta improved\n",
    "        for i in range(len(solution)): # Per ogni nodo della soluzione corrente\n",
    "            for j in range(k): # Per ogni possibile cluster in cui spostare il nodo\n",
    "                if solution[i] != j: # Se il nodo non è già nel cluster j\n",
    "                    new_solution = solution[:] # Copia profonda della soluzione corrente\n",
    "                    new_solution[i] = j # Sposta il nodo i nel cluster j (genera una nuova soluzione)\n",
    "                    current_fitness = fitness(new_solution, graph, k) # Calcola il fitness della nuova soluzione\n",
    "                    if current_fitness < best_fitness:\n",
    "                        best_solution = new_solution[:]\n",
    "                        best_fitness = current_fitness\n",
    "                        improved = True  # abbiamo trovato un miglioramento\n",
    "\n",
    "    return best_solution\n",
    "\n",
    "\n",
    "def genetic_algorithm_with_local_search(graph, k, pop_size=100, gen_count=1000, mutation_rate=0.01, local_search_iters=100, local_search_fraction=0.1):\n",
    "    population = init_population(pop_size, len(graph.nodes()), k)\n",
    "    fitness_values = [fitness(ind, graph, k) for ind in population]\n",
    "    elitism_size = int(0.10 * len(population))\n",
    "    \n",
    "    for gen in range(gen_count):\n",
    "        new_population = []\n",
    "        elite_indices = sorted(range(len(fitness_values)), key=lambda x: fitness_values[x])[:elitism_size]\n",
    "        \n",
    "        for elite_index in elite_indices:\n",
    "            new_population.append(population[elite_index])\n",
    "            \n",
    "        while len(new_population) < pop_size:\n",
    "            parent1 = roulette_wheel_selection(population, fitness_values)\n",
    "            parent2 = roulette_wheel_selection(population, fitness_values)\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            \n",
    "            if random.random() < mutation_rate:\n",
    "                mutate(child1, k)\n",
    "            if random.random() < mutation_rate:\n",
    "                mutate(child2, k)\n",
    "            new_population.extend([child1, child2])\n",
    "\n",
    "        fitness_values = [fitness(ind, graph, k) for ind in population] # Calcola il fitness della nuova popolazione prima di applicare la ricerca locale\n",
    "\n",
    "        population = new_population # Aggiorna la popolazione con la nuova popolazione\n",
    "    \n",
    "        # Applica la ricerca locale a una frazione della popolazione\n",
    "        num_individuals_to_improve = int(pop_size * local_search_fraction) # Calcola il numero di individui da migliorare con la ricerca locale\n",
    "        for i in range(num_individuals_to_improve): # Per ogni individuo da migliorare con la ricerca locale\n",
    "            population[i] = local_search_with_max_iterations(population[i], graph, k, local_search_iters)\n",
    "\n",
    "    # Trova la soluzione migliore nella popolazione finale\n",
    "    best_individual = min(population, key=lambda ind: fitness(ind, graph, k)) # minimizza il fitness\n",
    "    clusters = [[] for _ in range(max(best_individual) + 1)] # inizializza i cluster vuoti\n",
    "    for i, cluster_id in enumerate(best_individual): # per ogni nodo della soluzione migliore trovata\n",
    "        clusters[cluster_id].append(i) # aggiungi il nodo al cluster corrispondente\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def local_search_with_max_iterations(solution, graph, k, max_iterations):\n",
    "    best_solution = solution\n",
    "    best_fitness = fitness(solution, graph, k)\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        improved_solution = local_search_best_improvement_genetic(best_solution, graph, k)\n",
    "        current_fitness = fitness(improved_solution, graph, k)\n",
    "        \n",
    "        if current_fitness < best_fitness:\n",
    "            best_solution = improved_solution\n",
    "            best_fitness = current_fitness\n",
    "        else:\n",
    "            break  \n",
    "\n",
    "    return best_solution\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_graph(graph, title, subplot_position=None, filename=None, colors=None):\n",
    "    plt.subplot(subplot_position) if subplot_position else plt.figure()\n",
    "    pos = nx.spring_layout(graph, seed=42)\n",
    "    labels = {node: str(node) for node in graph.nodes()}\n",
    "    node_colors = {node: 'skyblue' for node in graph.nodes()}\n",
    "\n",
    "    if colors:\n",
    "        color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "        for i, cluster in enumerate(colors):\n",
    "            color = next(color_cycle)\n",
    "            for node in cluster:\n",
    "                node_colors[int(node)] = color\n",
    "\n",
    "    nx.set_node_attributes(graph, node_colors, 'color')\n",
    "\n",
    "    nx.draw(graph, pos, with_labels=True, labels=labels, node_size=1000, node_color=list(node_colors.values()), \n",
    "            font_size=12, font_color='black', edge_color='gray', edgecolors='black', linewidths=1)\n",
    "\n",
    "    plt.title(title)\n",
    "    if filename and not subplot_position:\n",
    "        plt.savefig(filename)\n",
    "    if not subplot_position:\n",
    "        plt.show()\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.grafo = {}\n",
    "\n",
    "    def aggiungi_vertice(self, vertice):\n",
    "        if vertice not in self.grafo:\n",
    "            self.grafo[vertice] = []\n",
    "\n",
    "    def aggiungi_arco(self, u, v):\n",
    "        if u in self.grafo and v in self.grafo:\n",
    "            self.grafo[u].append(v)\n",
    "            self.grafo[v].append(u)\n",
    "\n",
    "    def rimuovi_vertice(self, vertice):\n",
    "        if vertice in self.grafo:\n",
    "            self.grafo.pop(vertice, None)\n",
    "            for nodo, adiacenti in self.grafo.items():\n",
    "                if vertice in adiacenti:\n",
    "                    adiacenti.remove(vertice)\n",
    "\n",
    "\n",
    "    def stampa_grafo(self):\n",
    "        for vertice in self.grafo:\n",
    "            adiacenti = \", \".join(str(v) for v in self.grafo[vertice])\n",
    "            print(f\"{vertice} -> {adiacenti}\")\n",
    "\n",
    "def converti_grafo_personalizzato_in_networkx(grafo_personalizzato):\n",
    "    G = nx.Graph()\n",
    "    for vertice, adiacenti in grafo_personalizzato.grafo.items():\n",
    "        G.add_node(vertice)\n",
    "        for adiacente in adiacenti:\n",
    "            G.add_edge(vertice, adiacente)\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "def divide_into_clusters_greedy(graph, k):\n",
    "    G_temp = graph.copy()\n",
    "    \n",
    "    # Se il grafo non ha pesi, calcoliamo la centralità degli archi come criterio\n",
    "    if not nx.get_edge_attributes(G_temp, 'weight'): \n",
    "        centrality = nx.edge_betweenness_centrality(G_temp) # Calcola la centralità degli archi per ogni arco del grafo G_temp e restituisce un dizionario con gli archi come chiavi e la centralità degli archi come valori.\n",
    "        nx.set_edge_attributes(G_temp, centrality, 'weight')  # Imposta gli attributi degli archi del grafo G_temp con i valori del dizionario centrality.\n",
    "\n",
    "    edges_sorted = sorted(G_temp.edges(data=True), key=lambda x: x[2]['weight'], reverse=True) # Ordina gli archi del grafo G_temp in ordine decrescente in base al peso.\n",
    "    \n",
    "    while len(list(nx.connected_components(G_temp))) < k and edges_sorted:  #controllo: Se il numero di componenti connesse in G_temp è inferiore a k e se ci sono ancora archi nell'elenco edges_sorted da considerare per la rimozione.\n",
    "        e = edges_sorted.pop(0) # Rimuove e restituisce l'elemento all'indice specificato dall'elenco edges_sorted. L'indice predefinito è -1, il che significa che l'ultimo elemento viene rimosso e restituito.\n",
    "        G_temp.remove_edge(*e[:2]) # Rimuove l'arco tra i nodi e[0] e e[1] dal grafo G_temp.\n",
    "\n",
    "        if len(list(nx.connected_components(G_temp))) > k:\n",
    "            G_temp.add_edge(*e[:2])\n",
    "            break\n",
    "\n",
    "    clusters = list(nx.connected_components(G_temp))\n",
    "\n",
    "    # Se abbiamo meno di k cluster, distribuisci i nodi rimanenti\n",
    "    while len(clusters) < k:\n",
    "        # Per semplicità, ho aggiunto nodi in modo casuale\n",
    "        #  i nodi vengono aggiunti al primo cluster fino a raggiungere k clusters. La scelta del nodo da aggiungere è casuale\n",
    "        \n",
    "        # set(itertools.chain.from_iterable(clusters)) l'espressione combina tutti gli elementi dei clusters in un unico set. \n",
    "        # next(iter(set(graph.nodes()) prendere \"un nodo qualsiasi\" dal grafo che non è già in un cluster\n",
    "        node_to_add = next(iter(set(graph.nodes()) - set(itertools.chain.from_iterable(clusters)))) # Prendi un nodo che non è ancora stato aggiunto\n",
    "        clusters[0].add(node_to_add)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def è_ammissibile(grafo, clusters, k):\n",
    "    # Vincolo 2: numero di sottografi = k\n",
    "    if len(clusters) != k:\n",
    "        return False\n",
    "\n",
    " # Vincolo 1: Ogni vertice in uno e solo uno dei sottografi\n",
    "    tutti_nodi = set(grafo.nodes())\n",
    "    union_clusters = set().union(*clusters)\n",
    "    missing_nodi = tutti_nodi - union_clusters\n",
    "    extra_nodi = union_clusters - tutti_nodi\n",
    "\n",
    "    if missing_nodi:\n",
    "        return False, f\"Vincolo 1 violato: i nodi {missing_nodi} non sono presenti in nessun cluster.\"\n",
    "    elif extra_nodi:\n",
    "        return False, f\"Vincolo 1 violato: i nodi {extra_nodi} sono presenti in più cluster.\"\n",
    "\n",
    "\n",
    "    # Vincolo 3 è implicito nella rappresentazione\n",
    "    for cluster in clusters:\n",
    "        if not cluster:  # Controlla se il cluster è vuoto\n",
    "            return False\n",
    "        sottografo = grafo.subgraph(cluster)\n",
    "        if not nx.is_connected(sottografo):\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# \n",
    "def calcola_dimensione_multicut(grafo, clusters):\n",
    "    dimensione_multicut = 0\n",
    "    for u, v in grafo.edges():\n",
    "        if not any(u in cluster and v in cluster for cluster in clusters):\n",
    "            dimensione_multicut += 1  \n",
    "    return dimensione_multicut\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy con Ricerca Locale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_graph(graph, title, subplot_position=None, filename=None, colors=None):\n",
    "    plt.subplot(subplot_position) if subplot_position else plt.figure()\n",
    "    pos = nx.spring_layout(graph, seed=42)\n",
    "    labels = {node: str(node) for node in graph.nodes()}\n",
    "    node_colors = {node: 'skyblue' for node in graph.nodes()}\n",
    "\n",
    "    if colors:\n",
    "        color_cycle = itertools.cycle(plt.cm.tab10.colors)\n",
    "        for i, cluster in enumerate(colors):\n",
    "            color = next(color_cycle)\n",
    "            for node in cluster:\n",
    "                node_colors[int(node)] = color\n",
    "\n",
    "    nx.set_node_attributes(graph, node_colors, 'color')\n",
    "\n",
    "    nx.draw(graph, pos, with_labels=True, labels=labels, node_size=1000, node_color=list(node_colors.values()), \n",
    "            font_size=12, font_color='black', edge_color='gray', edgecolors='black', linewidths=1)\n",
    "\n",
    "    plt.title(title)\n",
    "    if filename and not subplot_position:\n",
    "        plt.savefig(filename)\n",
    "    if not subplot_position:\n",
    "        plt.show()\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.grafo = {}\n",
    "\n",
    "    def aggiungi_vertice(self, vertice):\n",
    "        if vertice not in self.grafo:\n",
    "            self.grafo[vertice] = []\n",
    "\n",
    "    def aggiungi_arco(self, u, v):\n",
    "        if u in self.grafo and v in self.grafo:\n",
    "            self.grafo[u].append(v)\n",
    "            self.grafo[v].append(u)\n",
    "\n",
    "    def rimuovi_vertice(self, vertice):\n",
    "        if vertice in self.grafo:\n",
    "            self.grafo.pop(vertice, None)\n",
    "            for nodo, adiacenti in self.grafo.items():\n",
    "                if vertice in adiacenti:\n",
    "                    adiacenti.remove(vertice)\n",
    "\n",
    "\n",
    "    def stampa_grafo(self):\n",
    "        for vertice in self.grafo:\n",
    "            adiacenti = \", \".join(str(v) for v in self.grafo[vertice])\n",
    "            print(f\"{vertice} -> {adiacenti}\")\n",
    " \n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "def evaluate_solution(solution, graph): \n",
    "    counted_edges = set() # Inizializza un insieme vuoto di archi contati\n",
    "    for cluster in solution: # Per ogni cluster nella soluzione \n",
    "        if not isinstance(cluster, (set, list)):  # Verifica che il cluster sia un insieme o una lista\n",
    "            print(\"Errore: il cluster non è un insieme o una lista:\", cluster)\n",
    "            return float('inf')\n",
    "        for node in cluster: # Per ogni nodo nel cluster\n",
    "            neighbors = list(graph.neighbors(node)) # Trova i vicini del nodo nel grafo\n",
    "            for neighbor in neighbors: # Per ogni vicino del nodo \n",
    "                if neighbor not in cluster: # Se il vicino non è nel cluster \n",
    "                    edge = tuple(sorted([node, neighbor]))  # Ordina i nodi dell'arco in ordine crescente\n",
    "                    counted_edges.add(edge) # Aggiungi l'arco all'insieme di archi contati\n",
    "    return len(counted_edges) # Restituisci il numero di archi contati\n",
    "\n",
    "\n",
    "# Approccio best-improvment per migliorare la soluzione\n",
    "\n",
    "def local_search_improve(graph, initial_clusters):\n",
    "    def calculate_cost(clusters): # il costo è dato dal numero di collegamenti tra nodi in cluster diversi.\n",
    "        cost = 0\n",
    "        for cluster in clusters: # Per ogni cluster nella soluzione\n",
    "            for node in cluster: # Per ogni nodo nel cluster \n",
    "                for neighbor in graph.neighbors(node): # Per ogni vicino del nodo \n",
    "                    if neighbor not in cluster: # Se il vicino non è nel cluster\n",
    "                        cost += 1  # Aumenta il costo se il vicino non è nello stesso cluster\n",
    "        return cost\n",
    "# La funzione prova a scambiare quel nodo con ogni altro nodo in ogni altro cluster (sempre che quel cluster non sia il cluster originale del nodo).\n",
    "    clusters = [set(cluster) for cluster in initial_clusters] # Converte i cluster in insiemi per una maggiore efficienza\n",
    "    best_cost = calculate_cost(clusters) # Calcola il costo iniziale della soluzione\n",
    "\n",
    "    improvement = True # Flag per indicare se è stata trovata un'ulteriore soluzione migliore\n",
    "    while improvement: # Continua a cercare soluzioni migliori finché non ne trovi una\n",
    "        improvement = False # Resetta il flag di miglioramento\n",
    "\n",
    "        for i, cluster in enumerate(clusters): # Per ogni cluster nella soluzione \n",
    "            for node in list(cluster): # Per ogni nodo nel cluster\n",
    "                for j, target_cluster in enumerate(clusters): # Per ogni altro cluster nella soluzione\n",
    "                    if i != j: # Se il cluster non è lo stesso cluster\n",
    "                        if len(cluster) == 1:  # Se il nodo è l'unico nel suo cluster\n",
    "                            for target_node in list(target_cluster): # Per ogni nodo nel cluster di destinazione\n",
    "                                # Scambia i nodi\n",
    "                                new_clusters = clusters.copy() # Copia la soluzione corrente\n",
    "                                new_clusters[i] = {target_node} # Sposta il nodo nel cluster di destinazione\n",
    "                                new_clusters[j] = target_cluster - {target_node} | {node} # Sposta il nodo di destinazione nel cluster originale\n",
    "\n",
    "                                current_cost = calculate_cost(new_clusters) # Calcola il costo della nuova soluzione\n",
    "                                if current_cost < best_cost: # Se la nuova soluzione è migliore\n",
    "                                    best_cost = current_cost # Aggiorna il costo migliore\n",
    "                                    clusters = new_clusters # Aggiorna la soluzione migliore\n",
    "                                    improvement = True # Imposta il flag di miglioramento\n",
    "                        else:  # Se ci sono altri nodi nel cluster La funzione prova a spostare quel nodo in ogni altro cluster.\n",
    "                            # Sposta il nodo\n",
    "                            new_clusters = clusters.copy() # Copia la soluzione corrente\n",
    "                            new_clusters[i] = cluster - {node} # Rimuovi il nodo dal cluster originale\n",
    "                            new_clusters[j] = target_cluster | {node} # Aggiungi il nodo al cluster di destinazione\n",
    "\n",
    "                            current_cost = calculate_cost(new_clusters) # Calcola il costo della nuova soluzione\n",
    "                            if current_cost < best_cost: # Se la nuova soluzione è migliore\n",
    "                                best_cost = current_cost # Aggiorna il costo migliore\n",
    "                                clusters = new_clusters # Aggiorna la soluzione migliore\n",
    "                                improvement = True # Imposta il flag di miglioramento\n",
    "\n",
    "    return clusters # Restituisci la soluzione migliore trovata\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "def è_ammissibile(grafo, clusters, k):\n",
    "    # Vincolo 2: numero di sottografi = k\n",
    "    if len(clusters) != k:\n",
    "        return False, \"Vincolo 2 violato: il numero di sottografi non è k.\"\n",
    "\n",
    "  # Vincolo 1: Ogni vertice in uno e solo uno dei sottografi\n",
    "    tutti_nodi = set(grafo.nodes())\n",
    "    union_clusters = set().union(*clusters)\n",
    "    missing_nodi = tutti_nodi - union_clusters\n",
    "    extra_nodi = union_clusters - tutti_nodi\n",
    "\n",
    "    if missing_nodi:\n",
    "        return False, f\"Vincolo 1 violato: i nodi {missing_nodi} non sono presenti in nessun cluster.\"\n",
    "    elif extra_nodi:\n",
    "        return False, f\"Vincolo 1 violato: i nodi {extra_nodi} sono presenti in più cluster.\"\n",
    "\n",
    "\n",
    "    # Vincolo 3: verifica l'integrità e la connettività dei cluster\n",
    "    for cluster in clusters:\n",
    "        if not cluster:\n",
    "            return False, \"Vincolo 3 violato: cluster vuoto.\"\n",
    "        \n",
    "\n",
    "    return True, \"Ammissibile\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_search_first(graph, initial_clusters):\n",
    "    def calculate_cost(clusters): \n",
    "        cost = 0\n",
    "        for cluster in clusters:\n",
    "            for node in cluster:\n",
    "                for neighbor in graph.neighbors(node): \n",
    "                    if neighbor not in cluster:\n",
    "                        cost += 1  \n",
    "        return cost\n",
    "\n",
    "    clusters = [set(cluster) for cluster in initial_clusters]\n",
    "    best_cost = calculate_cost(clusters)\n",
    "\n",
    "    improvement = True\n",
    "    while improvement:\n",
    "        improvement = False\n",
    "        # Aggiungi un flag per segnalare se hai trovato un miglioramento durante l'esplorazione dell'intorno\n",
    "        found_improvement = False\n",
    "\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            for node in list(cluster):\n",
    "                for j, target_cluster in enumerate(clusters):\n",
    "                    if i != j:\n",
    "                        if len(cluster) == 1:  \n",
    "                            for target_node in list(target_cluster):\n",
    "                                new_clusters = clusters.copy()\n",
    "                                new_clusters[i] = {target_node}\n",
    "                                new_clusters[j] = target_cluster - {target_node} | {node}\n",
    "\n",
    "                                current_cost = calculate_cost(new_clusters)\n",
    "                                if current_cost < best_cost:\n",
    "                                    best_cost = current_cost\n",
    "                                    clusters = new_clusters\n",
    "                                    improvement = True\n",
    "                                    found_improvement = True\n",
    "                                    break  # Interrompi la ricerca non appena trovi un miglioramento\n",
    "                        else:\n",
    "                            new_clusters = clusters.copy()\n",
    "                            new_clusters[i] = cluster - {node}\n",
    "                            new_clusters[j] = target_cluster | {node}\n",
    "\n",
    "                            current_cost = calculate_cost(new_clusters)\n",
    "                            if current_cost < best_cost:\n",
    "                                best_cost = current_cost\n",
    "                                clusters = new_clusters\n",
    "                                improvement = True\n",
    "                                found_improvement = True\n",
    "                                break  # Interrompi la ricerca non appena trovi un miglioramento\n",
    "\n",
    "                    if found_improvement:  # Se hai trovato un miglioramento, interrompi il ciclo esterno\n",
    "                        break\n",
    "                if found_improvement:  # Interrompi il ciclo ancora più esterno\n",
    "                    break\n",
    "\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_representation(grafo):\n",
    "    node2vec = Node2Vec(grafo, dimensions=20, walk_length=16, num_walks=50, workers=8) \n",
    "    model = node2vec.fit(window=7, min_count=1)\n",
    "    nodes = list(grafo.nodes())\n",
    "    embeddings = np.array([model.wv[str(node)] for node in nodes])\n",
    "    return nodes, embeddings\n",
    "\n",
    "\n",
    "def kmeans_clustering(nodes, embeddings, k):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, max_iter=300, random_state=42)  # migliore inizializzazione e più iterazioni\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    print(kmeans.labels_)\n",
    "\n",
    "    clusters = {i: [] for i in range(k)}\n",
    "    for node, label in zip(nodes, labels):\n",
    "        clusters[label].append(node)\n",
    "        \n",
    "    centroids = kmeans.cluster_centers_\n",
    "    return clusters,centroids\n",
    "\n",
    "\n",
    "\n",
    "def minimize_multicut(grafo, clusters):\n",
    "    for _ in range(10):  # Numero fisso di iterazioni per la convergenza\n",
    "       for label, cluster in clusters.items():\n",
    "            for node in cluster:\n",
    "                external_edges = [(neighbor, clusters.get(neighbor, None)) for neighbor in grafo.neighbors(node) if clusters.get(neighbor) != label and clusters.get(neighbor) is not None]\n",
    "                if not external_edges:\n",
    "                    continue\n",
    "\n",
    "                target_label = max(set(tuple(lbl) for _, lbl in external_edges), key=lambda l: sum(1 for _, lbl in external_edges if tuple(lbl) == l))\n",
    "                if sum(1 for _, lbl in external_edges if lbl == target_label) > len(external_edges) / 2:\n",
    "                    clusters[node] = target_label\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def k_clustering_on_graph(grafo, k=4):\n",
    "    nodes, embeddings = node_representation(grafo)\n",
    "    clusters = kmeans_clustering(nodes, embeddings, k)\n",
    "    optimized_clusters = minimize_multicut(grafo, clusters)\n",
    "    return optimized_clusters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means con ricerca locale\n",
    "Best-improvment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbors(solution, k):\n",
    "    for cluster_index, cluster in enumerate(solution):\n",
    "        for node in cluster:\n",
    "            # Trova il cluster da cui prendere un nodo se questo cluster diventerebbe vuoto\n",
    "            donor_cluster_index = next((i for i in range(k) if i != cluster_index and len(solution[i]) > 1), None)\n",
    "            \n",
    "            for target_cluster_index in range(k):\n",
    "                if target_cluster_index != cluster_index:\n",
    "                    new_solution = deepcopy(solution)\n",
    "                    \n",
    "                    new_solution[cluster_index].remove(node)\n",
    "                    new_solution[target_cluster_index].append(node)\n",
    "                    \n",
    "                    # Se rimuovendo il nodo il cluster diventa vuoto, prendiamo un nodo da un altro cluster\n",
    "                    if donor_cluster_index is not None and len(new_solution[cluster_index]) == 0:\n",
    "                        donor_node = next(iter(new_solution[donor_cluster_index]))\n",
    "                        new_solution[donor_cluster_index].remove(donor_node)\n",
    "                        new_solution[cluster_index].append(donor_node)\n",
    "                    \n",
    "                    # Verifica che la nuova soluzione abbia ancora k cluster\n",
    "                    if len([cl for cl in new_solution if cl]) == k:\n",
    "                        yield new_solution\n",
    "\n",
    "    # Se ci sono nodi che non appartengono a nessun cluster, inseriscili in un cluster vuoto\n",
    "    empty_clusters = [i for i in range(k) if not solution[i]]\n",
    "    for node in range(len(solution)):\n",
    "        if not any(node in cluster for cluster in solution):\n",
    "            target_cluster_index = empty_clusters.pop(0) if empty_clusters else random.randint(0, k-1)\n",
    "            new_solution = deepcopy(solution)\n",
    "            new_solution[target_cluster_index].append(node)\n",
    "            yield new_solution\n",
    "\n",
    "\n",
    "            \n",
    "                    \n",
    "def evaluate(solution, graph): # calcola la dimensione del multicut\n",
    "    if not all(isinstance(cluster, (list, set, tuple)) for cluster in solution): # controllo che solution sia una lista di iterabili\n",
    "        raise ValueError(\"Si prevede che 'clusters' sia una lista di iterabili (list, set, tuple).\") # se non lo è, solleva un'eccezione\n",
    "    counted_edges = set() # creo un insieme vuoto di archi\n",
    "    \n",
    "    for cluster in solution:    # itero i cluster della soluzione corrente\n",
    "        for node in cluster:   # itero i nodi del cluster corrente\n",
    "            neighbors = list(graph.neighbors(node)) # trovo i nodi adiacenti a node (i vicini) e li metto in una lista\n",
    "            \n",
    "            for neighbor in neighbors: # itero i nodi adiacenti a node (i vicini)\n",
    "                if neighbor not in cluster: # se il vicino non è nel cluster corrente (se l'arco non è interno al cluster)\n",
    "                    edge = tuple(sorted([node, neighbor])) # ordino i nodi dell'arco in ordine crescente e creo una tupla (arco non orientato) \n",
    "                    counted_edges.add(edge) # aggiungo l'arco all'insieme degli archi contati (se l'arco è già presente, non viene aggiunto)\n",
    "\n",
    "    return len(counted_edges) # restituisco la dimensione del multicut (numero di archi contati)\n",
    "\n",
    "def best_improvement_local_search(initial_solution, graph, k):\n",
    "    current_solution = deepcopy(initial_solution)\n",
    "    current_value = evaluate(current_solution, graph)\n",
    "    \n",
    "    while True:\n",
    "        best_neighbor = None\n",
    "        best_value = current_value\n",
    "        \n",
    "        for neighbor_solution in neighbors(current_solution, k):\n",
    "            neighbor_value = evaluate(neighbor_solution, graph)\n",
    "            if neighbor_value < best_value:\n",
    "                best_neighbor = neighbor_solution\n",
    "                best_value = neighbor_value\n",
    "        \n",
    "        if best_neighbor is None:\n",
    "            break\n",
    "        \n",
    "        current_solution = best_neighbor\n",
    "        current_value = best_value\n",
    "\n",
    "    return current_solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Codice principale**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# istanze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calcola_dimensione_multicut(grafo, clusters):\n",
    "    if not all(isinstance(cluster, (list, set, tuple)) for cluster in clusters):\n",
    "        raise ValueError(\"Si prevede che 'clusters' sia una lista di iterabili (list, set, tuple).\")\n",
    "\n",
    "    multicut_size = 0\n",
    "\n",
    "    # Crea un dizionario per mappare i nodi ai loro cluster\n",
    "    etichette_cluster = {}\n",
    "    for etichetta, cluster in enumerate(clusters):\n",
    "        for nodo in cluster:\n",
    "            etichette_cluster[nodo] = etichetta\n",
    "\n",
    "    for arco in grafo.edges():\n",
    "        if etichette_cluster[arco[0]] != etichette_cluster[arco[1]]:\n",
    "            multicut_size += 1\n",
    "\n",
    "    return multicut_size\n",
    "\n",
    "def genera_grafo_random(n, p):\n",
    "    \n",
    "    # Genera un grafo random con n nodi che è garantito essere connesso.\n",
    "    # Ogni possibile arco aggiuntivo è presente con probabilità p.\n",
    "    \n",
    "    grafo = Graph()\n",
    "\n",
    "    # Aggiungiamo i vertici al grafo\n",
    "    for i in range(n):\n",
    "        grafo.aggiungi_vertice(i)\n",
    "\n",
    "    # Assicuriamoci che il grafo sia connesso creando una catena di nodi\n",
    "    for i in range(n-1):\n",
    "        grafo.aggiungi_arco(i, i+1)\n",
    "\n",
    "    # Aggiungiamo gli archi al grafo con probabilità p\n",
    "    for i in range(n):\n",
    "        for j in range(i+2, n):  # Iniziamo da i+2 perché i nodi i e i+1 sono già connessi\n",
    "            if random.random() < p:\n",
    "                grafo.aggiungi_arco(i, j)\n",
    "\n",
    "    return grafo\n",
    "\n",
    "\n",
    "def converti_a_networkx(grafo_personalizzato):\n",
    "    G = nx.Graph()\n",
    "    for nodo, adiacenti in grafo_personalizzato.grafo.items():\n",
    "        G.add_node(nodo)\n",
    "        for adiacente in adiacenti:\n",
    "            G.add_edge(nodo, adiacente)\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "def genera_istanze(n_istanze=40):\n",
    "    istanze = []\n",
    "    \n",
    "    # Utilizzando la funzione genera_grafo_random:\n",
    "    for _ in range(n_istanze):\n",
    "        n = random.randint(4, 200)  # numero di nodi\n",
    "        p = random.uniform(0.05, 0.3)  # probabilità di un arco tra due nodi\n",
    "        G_random_personalizzato = genera_grafo_random(n, p)\n",
    "        G_random = converti_a_networkx(G_random_personalizzato)  # Convertiamo il grafo personalizzato in uno di networkx\n",
    "        istanze.append(G_random)\n",
    "    \n",
    "    return istanze\n",
    "\n",
    "\n",
    "# es 30 istanze di cui 10 con 100 nodi 10 con 300 e 10 con 600\n",
    "\n",
    "\n",
    "def main():\n",
    "    k = 3 # Numero di clusters desiderato\n",
    "    istanze_di_test = genera_istanze( n_istanze=2)\n",
    "    \n",
    "    ammissibilita = {\n",
    "        'Louvain': [],\n",
    "        'Louvain con Ricerca Locale': [],\n",
    "        'Greedy': [],\n",
    "        'Greedy con Ricerca Locale best-improvment': [],\n",
    "        'Greedy con Ricerca Locale first-improvment': [],\n",
    "        'Algoritmo Genetico': [],\n",
    "        'Algoritmo Genetico con Ricerca Locale': [],\n",
    "        'K-means': [],\n",
    "        'K-means con Ricerca Locale': []\n",
    "    }\n",
    "\n",
    "    tempi_esecuzione = {\n",
    "        'Louvain': [],\n",
    "        'Louvain con Ricerca Locale': [],\n",
    "        'Greedy': [],\n",
    "        'Greedy con Ricerca Locale best-improvment': [],\n",
    "        'Greedy con Ricerca Locale first-improvment': [],\n",
    "        'Algoritmo Genetico': [],\n",
    "        'Algoritmo Genetico con Ricerca Locale': [],\n",
    "        'K-means': [],\n",
    "        'K-means con Ricerca Locale': []\n",
    "    }\n",
    "    \n",
    "    multicuts = {\n",
    "        'Louvain': [],\n",
    "        'Louvain con Ricerca Locale': [],\n",
    "        'Greedy': [],\n",
    "        'Greedy con Ricerca Locale best-improvment': [],\n",
    "        'Greedy con Ricerca Locale first-improvment': [],\n",
    "        'Algoritmo Genetico': [],\n",
    "        'Algoritmo Genetico con Ricerca Locale': [],\n",
    "        'K-means': [],\n",
    "        'K-means con Ricerca Locale': []\n",
    "    }\n",
    "\n",
    "\n",
    "    for idx, grafo in enumerate(istanze_di_test):\n",
    "        print(f\"Istanza {idx+1}:\")\n",
    "\n",
    "        # Louvain\n",
    "        start_time = time.time()\n",
    "        partition = divide_into_clusters_fixed_k(grafo, k)\n",
    "        \n",
    "        end_time_louvain = time.time() - start_time\n",
    "        clusters_louvain = [cluster for cluster in partition]\n",
    "\n",
    "\n",
    "        multicut_louvain = calcola_dimensione_multicut(grafo, clusters_louvain)\n",
    "        ammissibile_louvain = è_ammissibile(grafo, clusters_louvain, k)\n",
    "        if not ammissibile_louvain:\n",
    "            print(\"Louvain ha prodotto una soluzione non ammissibile!\")\n",
    "        #plot_graph(grafo, f\"Istanza {idx+1} - Algoritmo Louvain\", filename=f'output_istanza_{idx+1}_louvain.png', colors=clusters_louvain)\n",
    "        print(f\"  - Louvain: {end_time_louvain:.4f} secondi, Dimensione Multicut: {multicut_louvain}, Ammissibile: {ammissibile_louvain}\")\n",
    "        \n",
    "        # Louvain con ricerca Locale\n",
    "        clusters_louvain_ricerca_locale = [set(cluster) for cluster in clusters_louvain] # prendi l'output della tua funzione Louvain come inizio\n",
    "        start_time = time.time()\n",
    "        final_clusters_3 = louvain_with_local_search(grafo, clusters_louvain ,k)\n",
    "        end_time_louvain_ricerca_locale = time.time() - start_time\n",
    "        \n",
    "        multicut_louvain_ricerca_locale = evaluate_solution(final_clusters_3, grafo)  \n",
    "        ammissibile_louvain_ricerca_locale, motivo = è_ammissibile(grafo, final_clusters_3, k)\n",
    "        if not ammissibile_louvain_ricerca_locale: \n",
    "            print(f\"Louvain con Ricerca Locale ha prodotto una soluzione non ammissibile! Motivo: {motivo}\")\n",
    "        #plot_graph(grafo, f\"Istanza {idx+1} - Louvain con ricerca Locale\", filename=f'output_istanza_{idx+1}_louvain_ricerca_locale.png', colors=final_clusters_3)\n",
    "        print(f\"  - Louvain con Ricerca Locale: {end_time_louvain_ricerca_locale:.4f} secondi, Dimensione Multicut: {multicut_louvain_ricerca_locale}, Ammissibile: {ammissibile_louvain_ricerca_locale}\")\n",
    "        \n",
    "        \"\"\"        # Louvain con ricerca Locale\n",
    "        clusters_louvain_ricerca_locale = [set(cluster) for cluster in clusters_louvain] # prendi l'output della tua funzione Louvain come inizio\n",
    "        start_time = time.time()\n",
    "        final_clusters_3 = local_search_optimize_modularity(grafo, clusters_louvain, k)\n",
    "        end_time_louvain_ricerca_locale = time.time() - start_time\n",
    "        \n",
    "        multicut_louvain_ricerca_locale = evaluate_solution(final_clusters_3, grafo)  \n",
    "        ammissibile_louvain_ricerca_locale, motivo = è_ammissibile(grafo, final_clusters_3, k)\n",
    "        if not ammissibile_louvain_ricerca_locale: \n",
    "            print(f\"Louvain con Ricerca Locale ha prodotto una soluzione non ammissibile! Motivo: {motivo}\")\n",
    "        # plot_graph(grafo, f\"Istanza {idx+1} - Louvain con ricerca Locale\", filename=f'output_istanza_{idx+1}_louvain_ricerca_locale.png', colors=final_clusters_3)\n",
    "        print(f\"  - Louvain con Ricerca Locale: {end_time_louvain_ricerca_locale:.4f} secondi, Dimensione Multicut: {multicut_louvain_ricerca_locale}, Ammissibile: {ammissibile_louvain_ricerca_locale}\")\n",
    "        \n",
    "         \"\"\"\n",
    "\n",
    "        # Greedy\n",
    "        start_time = time.time()\n",
    "        clusters_greedy = divide_into_clusters_greedy(grafo, k)\n",
    "\n",
    "        end_time_greedy = time.time() - start_time\n",
    "        multicut_greedy = calcola_dimensione_multicut(grafo, clusters_greedy)\n",
    "        ammissibile_greedy = è_ammissibile(grafo, clusters_greedy, k)\n",
    "        if not ammissibile_greedy:\n",
    "            print(\"Greedy ha prodotto una soluzione non ammissibile!\")\n",
    "        #plot_graph(grafo, f\"Istanza {idx+1} - Greedy\", filename=f'output_istanza_{idx+1}_greedy.png', colors=clusters_greedy)\n",
    "        print(f\"  - Greedy: {end_time_greedy:.4f} secondi, Dimensione Multicut: {multicut_greedy}, Ammissibile: {ammissibile_greedy}\")\n",
    "\n",
    "        # Greedy con Ricerca Locale approccio best-improvment\n",
    "        clusters_greedy_ricerca_locale_1 = [set(cluster) for cluster in clusters_greedy] # prendi l'output della tua funzione greedy come inizio\n",
    "        start_time = time.time()\n",
    "        final_clusters_1 = local_search_improve(grafo, clusters_greedy_ricerca_locale_1)\n",
    "        end_time_greedy_ricerca_locale_1 = time.time() - start_time\n",
    "\n",
    "        multicut_greedy_ricerca_locale_1 = evaluate_solution(final_clusters_1, grafo)  # usa final_clusters non clusters_greedy_ricerca_locale\n",
    "        ammissibile_greedy_ricerca_locale_1, motivo = è_ammissibile(grafo, final_clusters_1, k) # usa final_clusters non clusters_greedy_ricerca_locale e k è stato sostituito con 3 perché hai fornito quel valore prima\n",
    "        if not ammissibile_greedy_ricerca_locale_1:\n",
    "            print(f\"Greedy con Ricerca Locale best-improvment ha prodotto una soluzione non ammissibile! Motivo: {motivo}\")\n",
    "        #plot_graph(grafo, f\"Istanza {idx+1} - Greedy con Ricerca Locale\", filename=f'output_istanza_{idx+1}_greedy_ricerca_locale.png', colors=final_clusters_1)\n",
    "        print(f\"  - Greedy con Ricerca Locale best-improvment: {end_time_greedy_ricerca_locale_1:.4f} secondi, Dimensione Multicut: {multicut_greedy_ricerca_locale_1}, Ammissibile: {ammissibile_greedy_ricerca_locale_1}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Greedy con Ricerca Locale approccio first-improvment\n",
    "        clusters_greedy_ricerca_locale_2 = [set(cluster) for cluster in clusters_greedy] # prendi l'output della tua funzione greedy come inizio\n",
    "        start_time = time.time()\n",
    "        final_clusters_2 = local_search_first(grafo, clusters_greedy_ricerca_locale_2)\n",
    "        end_time_greedy_ricerca_locale_2 = time.time() - start_time\n",
    "\n",
    "        multicut_greedy_ricerca_locale_2 = evaluate_solution(final_clusters_2, grafo)  # usa final_clusters non clusters_greedy_ricerca_locale\n",
    "        ammissibile_greedy_ricerca_locale_2, motivo = è_ammissibile(grafo, final_clusters_2, k) # usa final_clusters non clusters_greedy_ricerca_locale e k è stato sostituito con 3 perché hai fornito quel valore prima\n",
    "        if not ammissibile_greedy_ricerca_locale_2:\n",
    "            print(f\"Greedy con Ricerca Locale first-improvment ha prodotto una soluzione non ammissibile! Motivo: {motivo}\")\n",
    "        #plot_graph(grafo, f\"Istanza {idx+1} - Greedy con Ricerca Locale\", filename=f'output_istanza_{idx+1}_greedy_ricerca_locale.png', colors=final_clusters_2)\n",
    "        print(f\"  - Greedy con Ricerca Locale first-improvment: {end_time_greedy_ricerca_locale_2:.4f} secondi, Dimensione Multicut: {multicut_greedy_ricerca_locale_2}, Ammissibile: {ammissibile_greedy_ricerca_locale_2}\")\n",
    "\n",
    "        # Algoritmo Genetico \n",
    "        start_time = time.time()\n",
    "        clusters_genetico = genetic_algorithm(grafo, k)\n",
    "        end_time_genetico = time.time() - start_time\n",
    "        multicut_genetico = calcola_dimensione_multicut(grafo, clusters_genetico)\n",
    "        ammissibile_genetico = è_ammissibile(grafo, clusters_genetico, k)\n",
    "        if not ammissibile_genetico:\n",
    "            print(\"L'Algoritmo Genetico ha prodotto una soluzione non ammissibile!\")\n",
    "        #plot_graph(grafo, f\"Istanza {idx+1} - Algoritmo Genetico\", filename=f'output_istanza_{idx+1}_genetico.png', colors=clusters_genetico)\n",
    "        print(f\"  - Algoritmo Genetico: {end_time_genetico:.4f} secondi, Dimensione Multicut: {multicut_genetico}, Ammissibile: {ammissibile_genetico}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # algoritmo genetico con ricerca locale\n",
    "        pop_size = 100\n",
    "        gen_count = 1000\n",
    "        mutation_rate = 0.01\n",
    "        local_search_iters = 100\n",
    "        clusters_genetico_ricerca_locale = [set(cluster) for cluster in clusters_genetico]\n",
    "        start_time = time.time()\n",
    "        final_clusters_4 = genetic_algorithm_with_local_search(grafo, k, pop_size, gen_count, mutation_rate, local_search_iters)\n",
    "        end_time_genetico_ricerca_locale = time.time() - start_time\n",
    "        multicut_genetico_ricerca_locale = evaluate_solution(final_clusters_4, grafo)\n",
    "        ammissibile_genetico_ricerca_locale, motivo = è_ammissibile(grafo, final_clusters_4, k)\n",
    "        if not ammissibile_genetico_ricerca_locale:\n",
    "            print(f\"Algoritmo Genetico con ricerca Locale ha prodotto una soluzione non ammissibile! Motivo: {motivo}\")\n",
    "        #plot_graph(grafo, f\"Istanza {idx+1} - Algoritmo Genetico con ricerca Locale\", filename=f'output_istanza_{idx+1}_genetico_ricerca_locale.png', colors=final_clusters_4)\n",
    "        print(f\"  - Algoritmo Genetico con ricerca Locale: {end_time_genetico_ricerca_locale:.4f} secondi, Dimensione Multicut: {multicut_genetico_ricerca_locale}, Ammissibile: {ammissibile_genetico_ricerca_locale}\")\n",
    "       \n",
    "       \n",
    "        # K-means \n",
    "        start_time = time.time()\n",
    "\n",
    "        nodes, embeddings = node_representation(grafo)\n",
    "        clusters_dict, centroids= kmeans_clustering(nodes, embeddings, k)\n",
    "        print(centroids)\n",
    "        \n",
    "        clusters_list = [clusters_dict[key] for key in clusters_dict]\n",
    "        optimized_clusters = minimize_multicut(grafo, clusters_dict)\n",
    "        optimized_clusters_list = [optimized_clusters[key] for key in optimized_clusters]\n",
    "        end_time_kmeans = time.time() - start_time\n",
    "\n",
    "        multicut_kmeans = calcola_dimensione_multicut(grafo, optimized_clusters_list)\n",
    "        ammissibile_kmeans, msg_kmeans = è_ammissibile(grafo, optimized_clusters_list, k)\n",
    "\n",
    "        if not ammissibile_kmeans:\n",
    "            print(\"K-means ha prodotto una soluzione non ammissibile!\")\n",
    "\n",
    "        #splot_graph(grafo, f\"Istanza {idx+1} - K-means\", filename=f'output_istanza_{idx+1}_kmeans.png', colors=optimized_clusters_list)\n",
    "\n",
    "        print(f\"  - K-means: {end_time_kmeans:.4f} secondi, Dimensione Multicut: {multicut_kmeans}, Ammissibile: {msg_kmeans}\")\n",
    "\n",
    "        # k_means_on_graph con Ricerca Locale\n",
    "        start_time = time.time()\n",
    "        clusters_ricerca_locale_list = best_improvement_local_search(clusters_list.copy(), grafo, k)\n",
    "        end_time_k_means_ricerca_locale = time.time() - start_time\n",
    "\n",
    "        multicut_k_means_ricerca_locale = calcola_dimensione_multicut(grafo, clusters_ricerca_locale_list)\n",
    "        ammissibile_k_means_ricerca_locale = è_ammissibile(grafo, clusters_ricerca_locale_list, k)\n",
    "\n",
    "        \n",
    "        if not ammissibile_k_means_ricerca_locale:\n",
    "            print(\"k_means_on_graph con Ricerca Locale ha prodotto una soluzione non ammissibile!\")\n",
    "        \n",
    "        #plot_graph(grafo, f\"Istanza {idx+1} - k_means_on_graph con Ricerca Locale\", filename=f'output_istanza_{idx+1}_k_means_ricerca_locale.png', colors=clusters_ricerca_locale_list)\n",
    "        print(f\"  - k_means_on_graph con Ricerca Locale: {end_time_k_means_ricerca_locale:.4f} secondi, Dimensione Multicut: {multicut_k_means_ricerca_locale}, Ammissibile: {ammissibile_k_means_ricerca_locale[1]}\")\n",
    "\n",
    "        # After each algorithm's solution and time measurement\n",
    "        \n",
    "        ammissibilita['Louvain'].append(ammissibile_louvain)\n",
    "        ammissibilita['Louvain con Ricerca Locale'].append(ammissibile_louvain_ricerca_locale)\n",
    "        ammissibilita['Greedy'].append(ammissibile_greedy)\n",
    "        ammissibilita['Greedy con Ricerca Locale best-improvment'].append(ammissibile_greedy_ricerca_locale_1)\n",
    "        ammissibilita.setdefault('Greedy con Ricerca Locale first-improvment', []).append(ammissibile_greedy_ricerca_locale_2)\n",
    "        ammissibilita['Algoritmo Genetico'].append(ammissibile_genetico)\n",
    "        ammissibilita['Algoritmo Genetico con Ricerca Locale'].append(ammissibile_genetico_ricerca_locale)\n",
    "        ammissibilita['K-means'].append(ammissibile_kmeans)\n",
    "        ammissibilita['K-means con Ricerca Locale'].append(ammissibile_k_means_ricerca_locale[0])\n",
    "        \n",
    "        tempi_esecuzione['Louvain'].append(end_time_louvain)\n",
    "        tempi_esecuzione['Louvain con Ricerca Locale'].append(end_time_louvain_ricerca_locale)\n",
    "        tempi_esecuzione['Greedy'].append(end_time_greedy)\n",
    "        tempi_esecuzione['Greedy con Ricerca Locale best-improvment'].append(end_time_greedy_ricerca_locale_1)\n",
    "        tempi_esecuzione['Greedy con Ricerca Locale first-improvment'].append(end_time_greedy_ricerca_locale_2)\n",
    "        tempi_esecuzione['Algoritmo Genetico'].append(end_time_genetico)\n",
    "        tempi_esecuzione['Algoritmo Genetico con Ricerca Locale'].append(end_time_genetico_ricerca_locale)\n",
    "        tempi_esecuzione['K-means'].append(end_time_kmeans)\n",
    "        tempi_esecuzione['K-means con Ricerca Locale'].append(end_time_k_means_ricerca_locale)\n",
    "        \n",
    "        multicuts['Louvain'].append(multicut_louvain)\n",
    "        multicuts['Louvain con Ricerca Locale'].append(multicut_louvain_ricerca_locale)\n",
    "        multicuts['Greedy'].append(multicut_greedy)\n",
    "        multicuts['Greedy con Ricerca Locale best-improvment'].append(multicut_greedy_ricerca_locale_1)\n",
    "        multicuts['Greedy con Ricerca Locale first-improvment'].append(multicut_greedy_ricerca_locale_2)\n",
    "        multicuts['Algoritmo Genetico'].append(multicut_genetico)\n",
    "        multicuts['Algoritmo Genetico con Ricerca Locale'].append(multicut_genetico_ricerca_locale)\n",
    "        multicuts['K-means'].append(multicut_kmeans)\n",
    "        multicuts['K-means con Ricerca Locale'].append(multicut_k_means_ricerca_locale)\n",
    "        \n",
    "    #for key, value in ammissibilita.items():\n",
    "        #print(key, value)\n",
    "\n",
    "    # Calculate the efficiencies\n",
    "    efficienze = {\n",
    "        key: (1 - sum(1 for sub_tuple in value if isinstance(sub_tuple, (tuple, list)) and sub_tuple[0]) / len(istanze_di_test)) * 100\n",
    "        for key, value in ammissibilita.items()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the efficiencies dictionary\n",
    "    efficienze = {\n",
    "        'Louvain': 0,\n",
    "        'Louvain con Ricerca Locale': 0,\n",
    "        'Greedy': 0,\n",
    "        'Greedy con Ricerca Locale best-improvment': 0,\n",
    "        'Greedy con Ricerca Locale first-improvment': 0,\n",
    "        'Algoritmo Genetico': 0,\n",
    "        'Algoritmo Genetico con Ricerca Locale': 0,\n",
    "        'K-means': 0  ,\n",
    "        'K-means con Ricerca Locale': 0\n",
    "    }\n",
    "\n",
    "    for idx, grafo in enumerate(istanze_di_test):\n",
    "        #print(f\"Istanza {idx+1}:\")\n",
    "\n",
    "\n",
    "        efficienze['Louvain'] += end_time_louvain\n",
    "        efficienze['Louvain con Ricerca Locale'] += end_time_louvain_ricerca_locale\n",
    "        efficienze['Greedy'] += end_time_greedy\n",
    "        efficienze['Greedy con Ricerca Locale best-improvment'] += end_time_greedy_ricerca_locale_1\n",
    "        efficienze['Greedy con Ricerca Locale first-improvment'] += end_time_greedy_ricerca_locale_2\n",
    "        efficienze['Algoritmo Genetico'] += end_time_genetico\n",
    "        efficienze['Algoritmo Genetico con Ricerca Locale'] += end_time_genetico_ricerca_locale\n",
    "        efficienze['K-means'] += end_time_kmeans\n",
    "        efficienze['K-means con Ricerca Locale'] += end_time_k_means_ricerca_locale\n",
    "\n",
    "\n",
    "    # Normalizza i tempi per trasformarli in efficienze\n",
    "    total_time = sum(efficienze.values())\n",
    "    for key in efficienze:\n",
    "        efficienze[key] = (1 - efficienze[key] / total_time) * 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    algoritmi = list(tempi_esecuzione.keys())\n",
    "    colori = ['blue', 'green', 'red', 'cyan', 'purple', 'yellow',  'magenta', 'pink', 'orange']\n",
    "\n",
    "    # Calcola il tempo medio di esecuzione per ogni algoritmo\n",
    "    tempi_medi = {algo: sum(tempi) / len(tempi) for algo, tempi in tempi_esecuzione.items()}\n",
    "    #tempi_medi = {algo: sum(tempi) / len(tempi) for algo, tempi in tempi_esecuzione.items()}\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.bar(tempi_medi.keys(), tempi_medi.values(), color=colori)\n",
    "    plt.ylabel('Tempo medio di esecuzione (s)')\n",
    "    plt.title('Tempi medi di esecuzione per algoritmo')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)  # Limita l'asse y a valori compresi tra 0 e 0.05\n",
    "    # plt.yticks(range(0, 101, 10))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    bar_width = 0.15\n",
    "    index = np.arange(len(istanze_di_test))\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for idx, algo in enumerate(algoritmi):\n",
    "        plt.bar(index + idx * bar_width, tempi_esecuzione[algo], bar_width, color=colori[idx], label=algo)\n",
    "\n",
    "    plt.xlabel('Numero Istanza')\n",
    "    plt.ylabel('Tempo di esecuzione (s)')\n",
    "    plt.title('Tempi di esecuzione per algoritmo per ciascuna istanza di test')\n",
    "    plt.xticks(index + bar_width * (len(algoritmi) - 1) / 2, range(1, len(istanze_di_test) + 1))\n",
    "    plt.ylim(0, 1)  # Limita l'asse y a valori compresi tra 0 e 0.05\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Grafico che mostra i tempi di esecuzione per ogni istanza\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for idx, algo in enumerate(algoritmi):\n",
    "        plt.plot(range(1, len(istanze_di_test)+1), tempi_esecuzione[algo], marker='o', color=colori[idx], label=algo)\n",
    "        \n",
    "    plt.xlabel('Numero di istanze')\n",
    "    plt.ylabel('Tempo di esecuzione (s)')\n",
    "    plt.title('Tempi di esecuzione per algoritmo per ogni istanza')\n",
    "    plt.xticks(range(1, len(istanze_di_test)+1))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Grafico che mostra il numero di soluzioni ammissibili per ogni algoritmo\n",
    "    conteggio_ammissibilita = {k: sum(1 for val in v if val == True or (isinstance(val, tuple) and val[0])) for k, v in ammissibilita.items()}\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.bar(conteggio_ammissibilita.keys(), conteggio_ammissibilita.values(), color=colori)\n",
    "    plt.ylabel('Numero di soluzioni ammissibili')\n",
    "    plt.title('Numero di soluzioni ammissibili per algoritmo')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(range(0, 10, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "  \n",
    "\n",
    "    # Grafico che mostra il dimensione del multiut per ogni istanza\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    for idx, algo in enumerate(algoritmi):\n",
    "        plt.plot(range(1, len(istanze_di_test) + 1), multicuts[algo], marker='o', color=colori[idx], label=algo)\n",
    "\n",
    "    plt.xlabel('Numero Istanza', fontsize=14)\n",
    "    plt.ylabel('Dimensione del Multicut', fontsize=14)\n",
    "    plt.title('Dimensione del Multicut per ogni algoritmo per ciascuna istanza di test', fontsize=16)\n",
    "    plt.xticks(range(1, len(istanze_di_test) + 1))\n",
    "\n",
    "    plt.legend(loc='upper right', fontsize=12)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Grafico a barre della dimensione del multicut per ogni istanza e per ogni algoritmo selezionato\n",
    "    bar_width = 0.15\n",
    "    index = np.arange(len(istanze_di_test))\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    for idx, algo in enumerate(algoritmi):\n",
    "        plt.bar(index + idx * bar_width, multicuts[algo], bar_width, color=colori[idx], label=algo)\n",
    "\n",
    "    plt.xlabel('Numero Istanza')\n",
    "    plt.ylabel('Dimensione del Multicut')\n",
    "    plt.title('Dimensione del Multicut per ogni algoritmo selezionato per ciascuna istanza di test')\n",
    "    plt.xticks(index + bar_width * (len(algoritmi) - 1) / 2, range(1, len(istanze_di_test) + 1))\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Lista degli algoritmi e dei colori selezionati\n",
    "    algoritmi_confrontati_1 = ['Greedy', 'Greedy con Ricerca Locale best-improvment','Greedy con Ricerca Locale first-improvment']\n",
    "    colori = ['green', 'red', 'blue']\n",
    "\n",
    "    # 1. Grafico che mostra la dimensione del multicut per ogni algoritmo selezionato per ciascuna delle istanze di test.\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for idx, algo in enumerate(algoritmi_confrontati_1):\n",
    "        plt.plot(range(1, len(istanze_di_test) + 1), multicuts[algo], marker='o', color=colori[idx], label=algo)\n",
    "\n",
    "    plt.xlabel('Numero Istanza')\n",
    "    plt.ylabel('Dimensione del Multicut')\n",
    "    plt.title('Dimensione del Multicut per ogni algoritmo selezionato per ciascuna istanza di test')\n",
    "    plt.xticks(range(1, len(istanze_di_test) + 1))\n",
    "    #plt.yticks(range(0, 70, 5))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    algoritmi_confrontati_2 = ['Louvain', 'Greedy','Algoritmo Genetico','K-means']\n",
    "    colori = ['green', 'red', 'blue', 'purple']\n",
    "\n",
    "    # 1. Grafico che mostra la dimensione del multicut per ogni algoritmo selezionato per ciascuna delle istanze di test.\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for idx, algo in enumerate(algoritmi_confrontati_2):\n",
    "        plt.plot(range(1, len(istanze_di_test) + 1), multicuts[algo], marker='o', color=colori[idx], label=algo)\n",
    "\n",
    "    plt.xlabel('Numero Istanza')\n",
    "    plt.ylabel('Dimensione del Multicut')\n",
    "    plt.title('Dimensione del Multicut per ogni algoritmo selezionato per ciascuna istanza di test')\n",
    "    plt.xticks(range(1, len(istanze_di_test) + 1))\n",
    "    #plt.yticks(range(0, 70, 5))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    algoritmi_confrontati_3 = ['Louvain con Ricerca Locale', 'Greedy con Ricerca Locale best-improvment','Greedy con Ricerca Locale first-improvment', 'Algoritmo Genetico con Ricerca Locale','K-means con Ricerca Locale']\n",
    "    colori = ['green', 'red', 'blue', 'purple', 'yellow']\n",
    "\n",
    "    # 1. Grafico che mostra la dimensione del multicut per ogni algoritmo selezionato per ciascuna delle istanze di test.\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for idx, algo in enumerate(algoritmi_confrontati_3):\n",
    "        plt.plot(range(1, len(istanze_di_test) + 1), multicuts[algo], marker='o', color=colori[idx], label=algo)\n",
    "\n",
    "    plt.xlabel('Numero Istanza')\n",
    "    plt.ylabel('Dimensione del Multicut')\n",
    "    plt.title('Dimensione del Multicut per ogni algoritmo selezionato per ciascuna istanza di test')\n",
    "    plt.xticks(range(1, len(istanze_di_test) + 1))\n",
    "    #plt.yticks(range(0, 70, 5))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
